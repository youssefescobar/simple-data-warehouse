A simple ELT Data pipeline that populates a datewarehouse.

Phase 1: Data source

Since i want it to be as realistic as possible ill design a OLTP db on postgres and then extract from it to simulate a working production db.

1. Inits: i have to get a docker container running postgres up to start designing and seeding.
so ill start with simple docker compose file and only make 1 service which is the docker container.

its done, to connect 
docker exec -it production_db psql -U admin -d xyz_store

1.2 design prod db: 
https://dbdiagram.io/
 
as realistic as possible, makes u write near DMBL to create tables and visualize them. so i can eidt and paste into psql on the spot.

the main idea of the "prod db" is i want it to be realistic so ill follow an OLTP design and make it 3nf

i made 2 different dbs, 1 with 12 tables that is kinda realistc, and another with for for ease of use, since this is a side project. 
ill try my best with the realsitc one else ill just swap for the simpler if stuff got out of hand.

desgin done. now running the schema.sql file using 
docker exec -i production_db psql -U admin -d xyz_store < schema.sql

made a seeder file and seeded the db

now that the prod is seeded and we have a semi realistic db to extract data from this marks the end of phase 1 

i added a script that automates the initalization and seeding of both dbs upon requst since its alot of work to just stop container, delete file, up container, apply schema, seed , confirm.


Phase 2: Warehouse set up

first ill add to the docker compose file another postgres service to simulate a datawarehouse. 

ill use the Medallion architecture

Bronze: Raw extracts
Silver: Cleaned/joined
Gold: Business metrics

ok the wh is up. 
creeted the 3 schemas 

i manually wrote the bronze schema since its just 4 tables. 



Phase 3: EL set up.

i have alot of options do pick and choose from like the extraction strategy, will it be incremental or a full refresh.
will i load it in batches or all at once. 

after a quick research ill go with an incremental, high water mark load. to simulate working enviroments

incremental: only new stuff
high water mark: using the created at and updated at to tell to be able to tell new rows from old ones

extract and load script is done

what was done is
1. GET HIGH WATERMARK from Warehouse (Bronze)
2. checking if their is something new, if no stop.
3. if yes select it all
4. create a staging table
5. bulk load the df into staging table
6. insert into bronze
7. drop staging



Phase 4: transform

installed dbt-postgres

connected it to the xyz_store_wh silver schema

ran dbt debug to confirm 


made a sources.yml to ref the already made bronze layer

made a silver layer with minimal cleaning, could be done better and ill find out later

made a gold layer consisting of a star schema fact, dims tables. 


Phase 5: orchestraction












