A simple ELT Data pipeline that populates a datewarehouse.

Phase 1: Data source

Since i want it to be as realistic as possible ill design a OLTP db on postgres and then extract from it to simulate a working production db.

1. Inits: i have to get a docker container running postgres up to start designing and seeding.
so ill start with simple docker compose file and only make 1 service which is the docker container.

its done, to connect 
docker exec -it production_db psql -U admin -d xyz_store

1.2 design prod db: 
https://dbdiagram.io/
 
as realistic as possible, makes u write near DMBL to create tables and visualize them. so i can eidt and paste into psql on the spot.

the main idea of the "prod db" is i want it to be realistic so ill follow an OLTP design and make it 3nf

i made 2 different dbs, 1 with 12 tables that is kinda realistc, and another with for for ease of use, since this is a side project. 
ill try my best with the realsitc one else ill just swap for the simpler if stuff got out of hand.

desgin done. now running the schema.sql file using 
docker exec -i production_db psql -U admin -d xyz_store < schema.sql

made a seeder file and seeded the db

now that the prod is seeded and we have a semi realistic db to extract data from this marks the end of phase 1 

i added a script that automates the initalization and seeding of both dbs upon requst since its alot of work to just stop container, delete file, up container, apply schema, seed , confirm.


Phase 2: Warehouse set up

first ill add to the docker compose file another postgres service to simulate a datawarehouse. 

ill use the Medallion architecture

Bronze: Raw extracts
Silver: Cleaned/joined
Gold: Business metrics

ok the wh is up. 



Phase 3: ELT pipeline.

i have alot of options do pick and choose from like the extraction strategy, will it be incremental or a full refresh.
will i load it in batches or all at once. 

after a quick research ill go with an incremental, full copy paste structure since the db isnt that big. 









